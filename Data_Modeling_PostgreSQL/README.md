# Sparkify's Data Modeling with PostgreSQL
## Introduction
- Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app.
- The Project sets up an ETL workflow that creates the data model in a Postgres database.

## Datasets
- **Song Dataset**: The first dataset is a subset of real data from the Million Song Dataset. [Million Song Dataset.](http://millionsongdataset.com/) Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
- **Log Dataset**: The second dataset consists of log files in JSON format and generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.

## Data Schema and Design
### Fact Table
- **songplays** - records in log data associated with song plays i.e. records with page NextSong.
  - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
 
 ### Dimension Tables
 - **users** - users in the app
   - user_id, first_name, last_name, gender, level
  - **time** - timestamps of records in songplays broekn down into specific units
    - start_time, hour, day, week, month, year, weekday
 - **artists** - artists in music database
   - artist_id, name, location, latitude, longitude
  - **songs** - songs in the music database
    - song_id, title, artist_id, year, duration
   
 ![](https://udacity-reviews-uploads.s3.us-west-2.amazonaws.com/_attachments/33760/1586916755/Song_ERD.png)
 
## Project Files
- **sql_queries.py** : contains SQL queries for dropping and creating fact and dimension tables. Also, contains insert queries to insert values into tables. 
- **create_tables.py**: contains code for setting up database. Running this file creates sparkifydb and also creates the fact and dimension tables
- **etl.ipynb**: a jupyter notebook to analyse dataset before loading
- **etl.py** - read and process the datasets (song_data and log_data)
- **test.ipynb**: a notebook to connect the postgres database and validate the data loaded
 ## Setup
 1. Python 3 required
 2. PostgreSQL 9.5 or above
 3. psycoppg2 - PostgreSQL database adapter for Python

## Program Execution
- run **create_tables.py** to allow the database to be created and connected. This file will also create the necessary tables for the project.
- run **etl.py** to process the data in the datasets# Data_Modelling_Postgres
